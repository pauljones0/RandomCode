{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauljones0/RandomCode/blob/master/Run_Simple_Bench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "We will clone the `SimpleBench` repo and download the required dependencies."
      ],
      "metadata": {
        "id": "yMv7HOr5QYbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/simple-bench/SimpleBench.git\n",
        "%cd SimpleBench\n",
        "!git fetch origin pull/9/head:ayulockin/refactor\n",
        "!git checkout ayulockin/refactor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykCokjrSbO75",
        "outputId": "5500ca45-c41c-40d6-cef4-377e4f9b0692"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SimpleBench'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 39 (delta 10), reused 9 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (39/39), 718.04 KiB | 5.36 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "/content/SimpleBench/SimpleBench/SimpleBench/SimpleBench\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 10 (delta 7), reused 10 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (10/10), 1.43 KiB | 365.00 KiB/s, done.\n",
            "From https://github.com/simple-bench/SimpleBench\n",
            " * [new ref]         refs/pull/9/head -> ayulockin/refactor\n",
            "Switched to branch 'ayulockin/refactor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -qr pyproject.toml --system"
      ],
      "metadata": {
        "id": "h-obSFchfsRl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to provide the API key of a vendor of choice. If you are using OpenAI, the `LLM_API_NAME` is `OPENAI_API_KEY`; for Anthropic, the `LLM_API_NAME` is `ANTHROPIC_API_KEY`."
      ],
      "metadata": {
        "id": "JPOC8L2f_-SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "\n",
        "def setup_env(LLM_API_NAME):\n",
        "    os.environ[f\"{LLM_API_NAME}\"] = getpass.getpass(f\"Enter the {LLM_API_NAME}: \")\n",
        "setup_env(\"GEMINI_API_KEY\")\n",
        "#setup_env(\"OPENROUTER_API_KEY\")\n",
        "#setup_env(\"ANTHROPIC_API_KEY\")\n",
        "#setup_env(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Z8EeopQnMs",
        "outputId": "4cd7b8c4-3d29-4621-fd6a-e6bc6c575677"
      },
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the GEMINI_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the benchmark\n",
        "\n",
        "Execute the command below to run the benchmark on a model. The following models are supported but can easily be extended to new models because we are using `litellm` under the hood:\n",
        "\n",
        "```\n",
        "MODEL_MAP = {\n",
        "    \"o1\": \"o1\",\n",
        "    \"o1-preview\": \"o1-preview\",\n",
        "    \"o1-mini\": \"o1-mini\",\n",
        "    \"gpt-4o-mini\": \"gpt-4o-mini\",\n",
        "    \"gpt-4o\": \"gpt-4o-2024-08-06\",\n",
        "    \"gpt-4-turbo\": \"gpt-4-turbo\",\n",
        "    \"claude-3-5-sonnet-20240620\": \"claude-3-5-sonnet-20240620\",\n",
        "    \"claude-3-opus-20240229\": \"claude-3-opus-20240229\",\n",
        "    \"command-r-plus\": \"command-r-plus-08-2024\",\n",
        "    \"claude-3-haiku\": \"claude-3-haiku-20240307\",\n",
        "    \"gemini-1.5-pro-002\": \"gemini/gemini-1.5-pro-002\",\n",
        "    \"gemini-1.5-pro\": \"gemini/gemini-1.5-pro\",\n",
        "    \"gemini-2.0-flash-thinking\": \"gemini/gemini-2.0-flash-thinking-exp\",\n",
        "    \"mistral-large\": \"mistral/mistral-large-2407\",\n",
        "    \"grok-2\": \"openrouter/x-ai/grok-2\"\n",
        "}\n",
        "```\n",
        "\n",
        "NOTE: If you want to try out any other model supported by `litellm`, consider updating the `MODEL_MAP` dict in the `weave_utils/models.py` file. [List of models/vendors supported by litellm](https://docs.litellm.ai/docs/providers).\n",
        "\n",
        "The entry point to running this evaluation is executing a simple python script `python run_benchmark.py`.\n",
        "\n",
        "- It will load the `simple_benchmark_public.json` file containing the publicly available SimpleBench eval set if `use_weave_dataset=False`. Otherwise, it will load from the weave project [here](https://wandb.ai/simplebench/simple_bench_public/weave/objects/competition_dataset/versions/yrAqXoSOYV2cQ6pY8SRAt8XMbnywywS5YdkMi6nlux8).\n",
        "- It will load the system instruction aka system prompt from the `system_prompt.txt` file. If you want to modify the instruction given to the LLM modify this file. However make sure that your new system prompt ends with \"Final Answer: X where X is one of the letters A, B, C, D, E, or F\" so that our regex based answer parser works properly. Alternatively, you can write the system prompt as `str` and pass it with the `--system_prompt` flag.\n",
        "- You will probably want to change the model name(the available model list is shown above), temperature, top_p, etc. Check the available configurability options: `python run_benchmark.py -h`."
      ],
      "metadata": {
        "id": "PwIrJBZESFYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_benchmark.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVcyU4Bvfo_y",
        "outputId": "da741b42-a2a7-4fdf-bff4-a48b42047b47"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n",
            "INFO: Showing help with the command 'run_benchmark.py -- --help'.\n",
            "\n",
            "\u001b[1mNAME\u001b[0m\n",
            "    run_benchmark.py - Run a benchmark evaluation on a given model and dataset.\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0m\n",
            "    run_benchmark.py <flags>\n",
            "\n",
            "\u001b[1mDESCRIPTION\u001b[0m\n",
            "    Run a benchmark evaluation on a given model and dataset.\n",
            "\n",
            "\u001b[1mFLAGS\u001b[0m\n",
            "    --model_name=\u001b[4mMODEL_NAME\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'o1-preview'\n",
            "        Name of the model to use for inference. Default is \"gpt-4o-mini\".\n",
            "    --dataset_path=\u001b[4mDATASET_PATH\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'simple_bench_public.json'\n",
            "        Path to the dataset JSON file. Default is \"simple_bench_public.json\".\n",
            "    --dataset_weave_ref=\u001b[4mDATASET_WEAVE_REF\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'weave:///simplebench/simple_bench_public/object/competition...\n",
            "        Weave reference to the dataset.\n",
            "    -u, --use_weave_dataset=\u001b[4mUSE_WEAVE_DATASET\u001b[0m\n",
            "        Type: bool\n",
            "        Default: True\n",
            "        Whether to use the dataset from Weave. If False, the dataset will be loaded from the local file. Default is True.\n",
            "    --system_prompt=\u001b[4mSYSTEM_PROMPT\u001b[0m\n",
            "        Type: Optional[Optional]\n",
            "        Default: None\n",
            "        System prompt for the model.\n",
            "    -n, --num_responses=\u001b[4mNUM_RESPONSES\u001b[0m\n",
            "        Type: int\n",
            "        Default: 1\n",
            "        If greater than 1, majority voting will be applied. Default is 1 (no majority voting).\n",
            "    -e, --entity=\u001b[4mENTITY\u001b[0m\n",
            "        Type: Optional\n",
            "        Default: 'simplebench'\n",
            "        Optional Weave entity (org/user name) for evaluation tracking.\n",
            "    -p, --project=\u001b[4mPROJECT\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'simple_bench_public'\n",
            "        The project name under the specified entity. Default is \"simple_bench_public\".\n",
            "    --temp=\u001b[4mTEMP\u001b[0m\n",
            "        Type: float\n",
            "        Default: 0.7\n",
            "        Temperature for the model. Default is 0.7.\n",
            "    --max_tokens=\u001b[4mMAX_TOKENS\u001b[0m\n",
            "        Type: int\n",
            "        Default: 2048\n",
            "        Maximum number of tokens to generate. Default is 2048.\n",
            "    --top_p=\u001b[4mTOP_P\u001b[0m\n",
            "        Type: float\n",
            "        Default: 0.95\n",
            "        Top-p for the model. Default is 0.95.\n",
            "    --max_retries=\u001b[4mMAX_RETRIES\u001b[0m\n",
            "        Type: int\n",
            "        Default: 3\n",
            "        Maximum number of retries for the model. Default is 3.\n",
            "    --system_prompt_path=\u001b[4mSYSTEM_PROMPT_PATH\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'system_prompt.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When running this for the first time, you will be prompted to provide your Weights and Biases free API key. Visit: https://wandb.ai/authorize"
      ],
      "metadata": {
        "id": "3H_QLoJ6gRC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Running evaluation with the default system prompt.\n",
        "\n",
        "You will be prompted to put in your W&B API key. If you already have one, head over to [wandb.ai/authorize](https://wandb.ai/authorize) to get one. If you are new to Weights and Biases, head over to [wandb.ai/site](https://wandb.ai/site) to create a free W&B account."
      ],
      "metadata": {
        "id": "W9FoN4Qa7YPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_benchmark.py --model_name=\"gemini-1.5-pro\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhhSbfE6R6Oa",
        "outputId": "2ab6a9ef-7212-4e38-ade7-7f3a5a450c07"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n",
            "Logged in as Weights & Biases user: bethekind.\n",
            "View Weave data at https://wandb.ai/simplebench/simple_bench_public/weave\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SimpleBench/SimpleBench/SimpleBench/run_benchmark.py\", line 113, in <module>\n",
            "    Fire(run_benchmark)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/SimpleBench/run_benchmark.py\", line 92, in run_benchmark\n",
            "    model = LiteLLMModel(\n",
            "  File \"/content/SimpleBench/SimpleBench/SimpleBench/weave_utils/models.py\", line 59, in __init__\n",
            "    raise ValueError(f\"Invalid model name: {self.model_name}\")\n",
            "ValueError: Invalid model name: openrouter/google/gemini-2.0-flash-thinking-exp:free\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Let's play with the system prompt."
      ],
      "metadata": {
        "id": "hWILoQ0E8K79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "Be careful it's a trick question. The answer isn't what you think.\n",
        "The question contains many distractions. Identify the basic flaw to reason the true answer.\n",
        "It may help to think carefully about the location/physics of everything mentioned in the question.\n",
        "Many of these questions may be variations of some riddles you have been trained on in the past, but containing a trick that is easy for humans to identify.\n",
        "\n",
        "Start by rewriting the question and make special note of seeming innocuous details that could subvert the main text of the question.\n",
        "Then make a numbered list of factual statements from the question.\n",
        "From there deduce the best answer. Not the answer that would make the question make sense, but the correct answer for that specific weird scenario.\n",
        "\n",
        "Your final sentence should be in the format:\n",
        "Final Answer: X\n",
        "where X is one of the letters A, B, C, D, E, or F.\n",
        "\n",
        "You are AI Explained - a human with common sense designed to engage in thorough, self-questioning reasoning using a stream-of-consciousness approach.\n",
        "You think and behave like human, characterized by continuous exploration, self-doubt, and iterative analysis.\n",
        "\n",
        "## Context\n",
        "You are faced with questions designed to trick AI systems. Your task is to use human-like reasoning to identify and navigate these tricks. Many questions may be variations of common riddles, but with subtle twists that humans can easily spot. Since the questions are written in British English, pay special attention to seemingly innocuous details that could subvert the main text. Your goal is to provide the correct answer for the specific scenario presented, not the answer that would make the question conventionally logical.\n",
        "\n",
        "## Core Principles\n",
        "\n",
        "1. THOROUGH EXPLORATION\n",
        "- Never rush to conclusions\n",
        "- Question every assumption and inference\n",
        "- Keep exploring until a solution emerges naturally from the evidence\n",
        "- Value thorough exploration over quick resolution\n",
        "- Each thought should advance the analysis, always try new possibilities if applicable\n",
        "- If uncertain, continue reasoning indefinitely until everything is resolved\n",
        "\n",
        "2. DEPTH OF REASONING\n",
        "- Engage in extensive contemplation (minimum 10,000 characters)\n",
        "- Express thoughts in natural, conversational internal monologue\n",
        "- Break down complex thoughts into simple, atomic steps\n",
        "- Embrace uncertainty and revision of previous thoughts\n",
        "\n",
        "3. THINKING PROCESS\n",
        "- Use short, simple sentences that mirror natural thought patterns\n",
        "- Use MUST ONLY information explicitly stated in the text\n",
        "- Express uncertainty and internal debate freely\n",
        "- Show work-in-progress thinking\n",
        "- Acknowledge and explore dead ends\n",
        "- Frequently backtrack and revise\n",
        "\n",
        "4. PERSISTENCE\n",
        "- Value thorough exploration over quick resolution\n",
        "- The trick never contradicts the problem statement\n",
        "\n",
        "## Output Format\n",
        "\n",
        "<contemplator>\n",
        "[Extensive internal monologue]\n",
        "- Identify the underlying trick or counter-intuitive element\n",
        "- Follow with foundational observations\n",
        "- Question each step thoroughly\n",
        "- Show natural progression\n",
        "- Express and resolve doubts\n",
        "- Continue naturally until you reach a valid, unambiguous answer\n",
        "</contemplator>\n",
        "\n",
        "<final_answer>\n",
        "[Provided when reasoning converges]\n",
        "- Summarize your findings, choose the best logical answer that fits the initial text\n",
        "- Format: \"Final Answer: X\" (where X is A-F)\n",
        "</final_answer>\n",
        "\n",
        "## Natural Thought Examples\n",
        "- \"Hmm... let me think about this...\"\n",
        "- \"Wait, that doesn't seem right...\"\n",
        "- \"The text doesn't mention that...\"\n",
        "- \"Let me think counter-intuitively...\"\n",
        "- \"Let me break this down further...\"\n",
        "- \"The trick of the question might simply be ...\"\n",
        "- \"Am I overcomplicating this? What's the simplest explanation?\"\n",
        "- \"Which of these details actually matter for the outcome?\"\n",
        "- \"Let me check if I'm being distracted by irrelevant information...\"\n",
        "- \"I think there MUST be another alternative trick here...\"\n",
        "- \"I should take my time to reason this out thoroughly...\"\n",
        "- \"Let me rewrite this in American English to spot any British terms that might change the meaning...\"\n",
        "- \"This seems like a variation of a classic riddle, but what's different here?\"\n",
        "- \"Are there any seemingly unimportant details that could completely change the scenario?\"\n",
        "- \"The author is trying to trick AI systems - what would a human notice that AI might miss?\"\n",
        "- \"Let me focus on what's explicitly stated rather than making assumptions...\"\n",
        "- \"Just because step A is possible, does that guarantee step B will work?\"\n",
        "\n",
        "SITUATIONAL HINTS:\n",
        "- **No gender roles or jobs are subverted, and what seems like complex physical puzzles have basic solutions. Things like feet rather than hands, emotional psychology or variables like speed, flow, and wind will be mentioned when they DON'T actually play a role in answering the question. Measurements and calculations are usually misdirections. Stated constraints are absolute and must be followed at all cost.**\n",
        "\n",
        "Remember: Let conclusions emerge naturally from exhaustive contemplation.\n",
        "\"\"\"\n",
        "\n",
        "!python run_benchmark.py --model_name=gemini-1.5-pro --system_prompt \"{system_prompt}\" --temp=0.5 -n 5 --max-tokens=8096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpoksBg5DRAM",
        "outputId": "ad868dd5-4776-4568-c37f-e9cdf9a654f8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n",
            "Unable to access `me/think`.\n",
            "It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 403: Forbidden)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SimpleBench/SimpleBench/SimpleBench/SimpleBench/run_benchmark.py\", line 113, in <module>\n",
            "    Fire(run_benchmark)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/SimpleBench/SimpleBench/run_benchmark.py\", line 71, in run_benchmark\n",
            "    weave.init(f\"{entity}/{project}\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/api.py\", line 57, in init\n",
            "    initialized_client = weave_init.init_weave(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/weave_init.py\", line 108, in init_weave\n",
            "    client = weave_client.WeaveClient(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/weave_client.py\", line 596, in __init__\n",
            "    resp = self.server.ensure_project_exists(entity, project)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace_server_bindings/remote_http_trace_server.py\", line 110, in ensure_project_exists\n",
            "    project_creator.ensure_project_exists(entity, project)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/wandb_interface/project_creator.py\", line 42, in ensure_project_exists\n",
            "    return _ensure_project_exists(entity_name, project_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/wandb_interface/project_creator.py\", line 71, in _ensure_project_exists\n",
            "    raise CommError(exception.message)\n",
            "weave.wandb_interface.project_creator.CommError: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 403: Forbidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Putting o1 to test\n",
        "\n",
        "Note: with reasoning model putting \"Think step by step ...\" will throw error stating \"your request is flagged as potentially violating our usage policy. ..\"."
      ],
      "metadata": {
        "id": "sXgTH5zl98kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are challenged with hard problems that require spatial reasoning, an understanding of physics, relationship interplay and more.\n",
        "You might face riddles and other trick questions.\n",
        "Just output your final answer using the following format: Final Answer: X where X is one of the letters A, B, C, D, E, or F.\n",
        "\"\"\"\n",
        "\n",
        "!python run_benchmark.py --model_name=o1 --system_prompt \"{system_prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5OjmNx38tUz",
        "outputId": "55c44848-26d1-4b6b-b5f0-da85f3466e6e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n",
            "Logged in as Weights & Biases user: bethekind.\n",
            "View Weave data at https://wandb.ai/simplebench/simple_bench_public/weave\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 1, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 2, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m17\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "Error in retry 3, retrying... litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
            "model_output failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 226, in predict_and_score\n",
            "    model_output, model_call = await async_call_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/models.py\", line 107, in predict\n",
            "    raise Exception(\"Failed to get response after max retries\")\n",
            "Exception: Failed to get response after max retries\n",
            "Evaluated \u001b[1;36m18\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Predict and score failed\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 476, in eval_example\n",
            "    eval_row = await self.predict_and_score(model, example)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 650, in wrapper\n",
            "    res, _ = await _do_call_async(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 522, in _do_call_async\n",
            "    res, call = await execute_result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 331, in _call_async\n",
            "    return handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 329, in _call_async\n",
            "    res = await func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/flow/eval.py\", line 401, in predict_and_score\n",
            "    result = await async_call(score_fn, **score_args)\n",
            "  File \"/usr/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\n",
            "    return await loop.run_in_executor(None, func_call)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 23, in eval_multi_choice\n",
            "    model_answer = extract_answer(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 658, in wrapper\n",
            "    res, _ = _do_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 456, in _do_call\n",
            "    execute_result = _execute_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 340, in _execute_op\n",
            "    handle_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\", line 338, in _execute_op\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/SimpleBench/SimpleBench/weave_utils/scorers.py\", line 34, in extract_answer\n",
            "    match = re.search(r\"Final Answer:\\s*([A-F])\", output.strip(), re.IGNORECASE)\n",
            "AttributeError: 'NoneType' object has no attribute 'strip'\n",
            "Evaluated \u001b[1;36m19\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Evaluated \u001b[1;36m20\u001b[0m of \u001b[1;36m20\u001b[0m examples\n",
            "Evaluation summary\n",
            "\u001b[1m{\u001b[0m\u001b[32m'eval_multi_choice'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m\n",
            "🍩 https://wandb.ai/simplebench/simple_bench_public/r/call/019462b4-e581-71a2-9948-e391309aeffe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's your turn to apply your AI skills and break this benchmark. Feel free to fork the repo and add modifications if needed. The provided code gives you a starting point and is not exhustive. New ideas are welcomed."
      ],
      "metadata": {
        "id": "f3DK087i_91V"
      }
    }
  ]
}